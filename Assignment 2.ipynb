{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: Bogdan Constantinescu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 262200\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y: 4600\n",
      "Type of y: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_spam\n",
    "X, y = load_spam()\n",
    "# TO DO: Print size and type of X and y\n",
    "print(\"Size of X: \" + str(X.size))\n",
    "print(\"Type of X: \" + str(type(X)))\n",
    "print(\"Size of y: \" + str(y.size))\n",
    "print(\"Type of y: \" + str(type(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "There are no null values\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "print(X.isna().sum().sum())\n",
    "print(\"There are no null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create X_small and y_small \n",
    "X_small, _, y_small, _ = train_test_split(X, y, test_size=0.95, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Data size  Training accuracy  Validation accuracy\n",
      "0                       X and y           0.927640             0.934783\n",
      "1  First two columns of X and y           0.611180             0.607246\n",
      "2           X_small and y_small           0.931677             0.956522\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = LogisticRegression(max_iter = 2000)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "model.fit(X_train, y_train)    \n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "train_accuracy_1 = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy_1 = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.iloc[:, :2], y, test_size=0.3, random_state=0)\n",
    "model.fit(X_train, y_train)    \n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "train_accuracy_2 = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy_2 = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_small, y_small, test_size=0.3, random_state=0)\n",
    "model.fit(X_train, y_train)    \n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "train_accuracy_3 = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy_3 = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "results = pd.DataFrame(columns=[\"Data size\", \"Training accuracy\", \"Validation accuracy\"])\n",
    "\n",
    "dataset_names = [\"X and y\", \"First two columns of X and y\", \"X_small and y_small\"]\n",
    "train_accuracy = [train_accuracy_1, train_accuracy_2, train_accuracy_3]\n",
    "test_accuracy = [test_accuracy_1, test_accuracy_2, test_accuracy_3]\n",
    "\n",
    "results[\"Data size\"] = dataset_names\n",
    "results[\"Training accuracy\"] = train_accuracy\n",
    "results[\"Validation accuracy\"] = test_accuracy\n",
    "\n",
    "print(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "\n",
    "1. The training accuracy tends to improve when more data is used. As can be seen when only using the first two columns of X we get a training accuracy of 60% whereas when we use the entire dataset we get a training accuracy of 93%. A caveat here is that when X_small was used to train the data, the training accuracy was higher (95%). A possible explanation for this is that the model overfit the data. For the validation accuracy, it is clear that the more data that is used to train the model, the higher the accuracy. This is visible as the validation accuracy goes up from 60% (first two columns of X) to 93% (entire dataset). \n",
    "The total number of data points used to train the model when only using the first two columns is 4600 * 2 = 9200. The total number of data points used to train the model with X_small is 4600 * 0.05 * 57 = 13110. The total number of data points used to train the model with the entire dataset is 4600 * 57 = 262200. As can be seen here, in relative terms, the number of data points used to train the model between X_small and using only the first two columns is almost the same yet the validation accuracy is much higher for X_small. This would seem to indicate that it is more important to have a wider range of unique values (X_small) to test rather than just having a lot of data from only two parameters (Two columns of X).\n",
    "2. In this case (determining whether an email is spam or not) a false positive represents an email that was flagged as being spam when in reality it is not. A false negative represents an email that was not flagged as spam when in reality it is. It is worse to have a false positive in this situation becuase this means an email that is not spam will appear in your spam folder and you might miss important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "\n",
    "The process I implemented was to use my current knowledge, previous notebooks from the labs that we have already done, as well as online resources such as the scikit website (https://www.scikit-yb.org/en/latest/api/datasets/spam.html). The majority of my code was sourced from similar examples covered in the labs as well as knowledge I accumulated from ENSF 593. I completed the steps in the exact order they were outlined in the notebook. I did not use generative AI for this assignment. A challenge I had was figuring out how to use train_test_split to only keep 5% of the overall data. The way I overcame this challenge was to use the VS code prompt that explains exactly what the method is and all of its parameters. In this way I determined how to use it effectively to accomplish only keeping 5% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 8240\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y: 1030\n",
      "Type of y: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "# TO DO: Print size and type of X and y\n",
    "from yellowbrick.datasets import load_concrete\n",
    "X, y = load_concrete()\n",
    "\n",
    "print(\"Size of X: \" + str(X.size))\n",
    "print(\"Type of X: \" + str(type(X)))\n",
    "print(\"Size of y: \" + str(y.size))\n",
    "print(\"Type of y: \" + str(type(y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "There are no null values\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "print(X.isna().sum().sum())\n",
    "print(\"There are no null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5041945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "lr = model.fit(X_train, y_train) \n",
    "y_train_pred = lr.predict(X_train)   \n",
    "y_pred = lr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "970c038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "mse_val = mean_squared_error(y_test, y_pred)\n",
    "r2_val= r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training accuracy</th>\n",
       "      <th>Validation accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>113.410826</td>\n",
       "      <td>93.624364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>0.606594</td>\n",
       "      <td>0.635277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Training accuracy  Validation accuracy\n",
       "MSE         113.410826            93.624364\n",
       "R2            0.606594             0.635277"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "results = pd.DataFrame(columns=[\"Training accuracy\", \"Validation accuracy\"], index = [\"MSE\", \"R2\"])\n",
    "results[\"Training accuracy\"] = [mse_train, r2_train]\n",
    "results[\"Validation accuracy\"] = [mse_val, r2_val]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
    " \n",
    "### Answer\n",
    "A linear model produced decent results for this dataset but not fantastic. First of all, the R2 score is fairly low indicating that the model is not a very good fit for the data. Furthermore, the MSE score is quite high which again is not indicative of a good model, as this means the model is not accurately predicting the values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "\n",
    "Most of the code was sourced from my own knowledge as well as the material that has been covered so far in the course (lab notebooks covering similar content). I completed the steps in the order that they are outlined. \n",
    "For this part of the assignment I did use generative AI (namely ChatGPT). I used it for clarification on how to use the linear regression model to predict my target vectors. I also used it to to show me how to calculate MSE and R2. I prompted it using the code I already had, and asked it to perform the MSE and R2 calculations. I did not have to modify the code at all.\n",
    "I did not encounter any real challenges for this part of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "\n",
    "For part 2 of the assignment there is no real pattern to determine as we are only using one model (linear regression) on one set of data, so there is nothing to compare.\n",
    "\n",
    "For part 1, I already touched upon the pattern in the Questions section. I will reiterate it below.\n",
    "\n",
    "The first pattern is that the more data used to train the model the better the validation accuracy.\n",
    "The second pattern I noticed is that a wider range of unique data improves the accuracy of the model more than the same amount of data points but from a smaller range of attributes (in this case two columns).\n",
    "\n",
    "For a more in depth answer, you can refer back to the Questions section of Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n",
    "I enjoyed this assignment because by working through all the steps I gained a much better understanding of the models themselves, how they work, and their output. I also learned more about the methods that we have at our disposal (like train_test_split) and what they fundamentally mean. This assignment also helped clarify the distinction between classification (logistic regression) modeling, which is discrete and linear regression which is continuous, as well as the methods we use to determine their model accuracy.\n",
    "I liked that the reflection questions made me actually think about what I was doing and how the models worked, instead of just plugging in the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge highest R2 value: 0.6353495924197798 with alpha value of: 100.0\n",
      "Lasso highest R2 value: 0.6370126170149091 with alpha value of: 10.0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "value = 0.001\n",
    "val_r = value\n",
    "val_l = value\n",
    "max_r = 0\n",
    "max_l = 0\n",
    "while value <= 100:\n",
    "    model_r = Ridge(alpha=value)\n",
    "    model_l = Lasso(alpha=value)\n",
    "    \n",
    "    r = model_r.fit(X_train, y_train) \n",
    "    l = model_l.fit(X_train, y_train)\n",
    "    y_train_pred_r = r.predict(X_train)   \n",
    "    y_pred_r = r.predict(X_test)\n",
    "    y_train_pred_l = l.predict(X_train)   \n",
    "    y_pred_l = l.predict(X_test)\n",
    "\n",
    "    mse_train_r = mean_squared_error(y_train, y_train_pred_r)\n",
    "    r2_train_r = r2_score(y_train, y_train_pred_r)\n",
    "    mse_val_r = mean_squared_error(y_test, y_pred_r)\n",
    "    r2_val_r = r2_score(y_test, y_pred_r)\n",
    "    if r2_val_r > max_r:\n",
    "        max_r = r2_val_r\n",
    "        val_r = value\n",
    "    mse_train_l = mean_squared_error(y_train, y_train_pred_l)\n",
    "    r2_train_l = r2_score(y_train, y_train_pred_l)\n",
    "    mse_val_l = mean_squared_error(y_test, y_pred_l)\n",
    "    r2_val_l = r2_score(y_test, y_pred_l)\n",
    "    if r2_val_l > max_l:\n",
    "        max_l = r2_val_l\n",
    "        val_l = value\n",
    "    results_r = pd.DataFrame(columns=[\"Training accuracy\", \"Validation accuracy\"], index = [\"MSE\", \"R2\"])\n",
    "    results_r[\"Training accuracy\"] = [mse_train_r, r2_train_r]\n",
    "    results_r[\"Validation accuracy\"] = [mse_val_r, r2_val_r]\n",
    "\n",
    "    results_l = pd.DataFrame(columns=[\"Training accuracy\", \"Validation accuracy\"], index = [\"MSE\", \"R2\"])\n",
    "    results_l[\"Training accuracy\"] = [mse_train_l, r2_train_l]\n",
    "    results_l[\"Validation accuracy\"] = [mse_val_l, r2_val_l]\n",
    "\n",
    "    value *= 10\n",
    "    #print(results_r)\n",
    "    #print(results_l)\n",
    "\n",
    "print(\"Ridge highest R2 value: \" + str(max_r) + \" with alpha value of: \" + str(val_r))\n",
    "print(\"Lasso highest R2 value: \" + str(max_l) + \" with alpha value of: \" + str(val_l))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "*ANSWER HERE*\n",
    "\n",
    "The lasso method gave me the highest R2 value of 0.637 with an alpha value of 10. This score is not very good as only 64% of the variability of the data is explained by the regression model. It might be good enough, as this data is concerning concrete mix parameters so there is probably some error tolerance allowed, but overall it is not a great model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
